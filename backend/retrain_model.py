import librosa
import numpy as np
from pathlib import Path
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

def extract_features(audio_path, sr=16000):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ 54 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –∞—É–¥–∏–æ"""
    y, _ = librosa.load(audio_path, sr=sr)
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    y = librosa.util.normalize(y)
    y, _ = librosa.effects.trim(y, top_db=20)
    y = librosa.effects.preemphasis(y, coef=0.97)
    
    # MFCC
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
    mfcc_mean = np.mean(mfcc, axis=1)
    mfcc_std = np.std(mfcc, axis=1)
    
    # Chroma
    chroma = librosa.feature.chroma_stft(y=y, sr=sr)
    chroma_mean = np.mean(chroma, axis=1)
    chroma_std = np.std(chroma, axis=1)
    
    # ZCR
    zcr = librosa.feature.zero_crossing_rate(y)
    zcr_mean = np.mean(zcr)
    zcr_std = np.std(zcr)
    
    # Spectral Centroid
    sc = librosa.feature.spectral_centroid(y=y, sr=sr)
    sc_mean = np.mean(sc) / sr
    sc_std = np.std(sc) / sr
    
    features = np.concatenate([
        mfcc_mean, mfcc_std,
        chroma_mean, chroma_std,
        [zcr_mean, zcr_std],
        [sc_mean, sc_std]
    ])
    
    return features

def retrain_models():
    """–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    print("="*60)
    print("–ù–ê–ß–ê–õ–û –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–Ø –ú–û–î–ï–õ–ï–ô")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    audio_dir = Path("./audio_samples")
    if not audio_dir.exists():
        print("–ü–∞–ø–∫–∞ —Å –∞—É–¥–∏–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return False
    
    X = []
    y = []
    
    print("\n –ó–∞–≥—Ä—É–∑–∫–∞ –∞—É–¥–∏–æ—Ñ–∞–π–ª–æ–≤...")
    for speaker_dir in audio_dir.iterdir():
        if speaker_dir.is_dir():
            speaker_name = speaker_dir.name
            audio_files = list(speaker_dir.glob("*.wav"))
            print(f"   üë§ {speaker_name}: {len(audio_files)} —Ñ–∞–π–ª–æ–≤")
            
            for audio_file in audio_files:
                try:
                    features = extract_features(audio_file)
                    X.append(features)
                    y.append(speaker_name)
                except Exception as e:
                    print(f"   –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {audio_file.name}: {e}")
    
    if len(X) == 0:
        print("\n–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
        return False
    
    X = np.array(X)
    y = np.array(y)
    
    unique_speakers = np.unique(y)
    print(f"\n–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(X)} –∑–∞–ø–∏—Å–µ–π –æ—Ç {len(unique_speakers)} –≥–æ–≤–æ—Ä—è—â–∏—Ö")
    print(f"   –ì–æ–≤–æ—Ä—è—â–∏–µ: {', '.join(unique_speakers)}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
    if len(X) < 10:
        print("\n–°–ª–∏—à–∫–æ–º –º–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–º–∏–Ω–∏–º—É–º 10 –∑–∞–ø–∏—Å–µ–π)")
    
    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
    label_encoder = LabelEncoder()
    y_encoded = label_encoder.fit_transform(y)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    test_size = 0.2 if len(X) > 20 else 0.1
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y_encoded, 
        test_size=test_size, 
        random_state=42, 
        stratify=y_encoded
    )
    
    print(f"\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} –∑–∞–ø–∏—Å–µ–π")
    print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} –∑–∞–ø–∏—Å–µ–π")
    
    # –û–±—É—á–µ–Ω–∏–µ Random Forest
    print("\n–û–±—É—á–µ–Ω–∏–µ Random Forest...")
    rf_model = RandomForestClassifier(
        n_estimators=200,
        max_depth=30,
        min_samples_split=2,
        random_state=42,
        n_jobs=-1
    )
    rf_model.fit(X_train, y_train)
    rf_score = rf_model.score(X_test, y_test)
    print(f"   Accuracy: {rf_score:.4f} ({rf_score*100:.2f}%)")
    
    # –û–±—É—á–µ–Ω–∏–µ SVM
    print("\n–û–±—É—á–µ–Ω–∏–µ SVM...")
    svm_model = SVC(kernel='rbf', C=10, gamma='scale', probability=True, random_state=42)
    svm_model.fit(X_train, y_train)
    svm_score = svm_model.score(X_test, y_test)
    print(f"   Accuracy: {svm_score:.4f} ({svm_score*100:.2f}%)")
    
    # –û–±—É—á–µ–Ω–∏–µ Logistic Regression
    print("\n–û–±—É—á–µ–Ω–∏–µ Logistic Regression...")
    lr_model = LogisticRegression(max_iter=1000, random_state=42)
    lr_model.fit(X_train, y_train)
    lr_score = lr_model.score(X_test, y_test)
    print(f"   Accuracy: {lr_score:.4f} ({lr_score*100:.2f}%)")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    print("\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
    models_dir = Path("./models")
    models_dir.mkdir(exist_ok=True)
    
    joblib.dump(rf_model, models_dir / "model_randomforest.pkl")
    joblib.dump(svm_model, models_dir / "model_svm.pkl")
    joblib.dump(lr_model, models_dir / "model_logisreg.pkl")
    joblib.dump(scaler, models_dir / "scaler.pkl")
    joblib.dump(label_encoder, models_dir / "label_encoder.pkl")
    
    print("\n" + "="*60)
    print("–ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–ï–®–ù–û!")
    print("="*60)
    print(f"\n–ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"   –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ –≥–æ–≤–æ—Ä—è—â–∏—Ö: {len(label_encoder.classes_)}")
    print(f"   –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {'Random Forest' if rf_score >= max(svm_score, lr_score) else 'SVM' if svm_score >= lr_score else 'Logistic Regression'}")
    print(f"   –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {np.mean([rf_score, svm_score, lr_score]):.4f}")
    
    return True

if __name__ == "__main__":
    success = retrain_models()
    exit(0 if success else 1)